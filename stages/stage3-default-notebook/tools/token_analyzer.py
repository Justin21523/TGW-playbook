# stages/stage3-default-notebook/token_analyzer.py
"""
TGW Token Analyzer & Optimizer
Token åˆ†æèˆ‡æœ€ä½³åŒ–å°ˆç”¨å·¥å…·

åŠŸèƒ½ï¼š
- Token ä½¿ç”¨æ•ˆç‡åˆ†æ
- æˆæœ¬è¨ˆç®—èˆ‡æœ€ä½³åŒ–å»ºè­°
- å¤šèªè¨€ tokenization æ¯”è¼ƒ
- Batch processing æœ€ä½³åŒ–
"""

import json
import re
import time
import requests
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import numpy as np


@dataclass
class TokenAnalysis:
    """Token åˆ†æçµæœ"""

    text: str
    token_count: int
    tokens: List[str]
    token_ids: List[int]
    char_count: int
    efficiency_ratio: float  # chars per token
    estimated_cost: float
    language: str
    complexity_score: float


class TGWTokenAnalyzer:
    """TGW Token åˆ†æå™¨"""

    def __init__(
        self,
        api_base: str = "http://localhost:5000",
        results_dir: str = "./results/token_analysis",
    ):
        self.api_base = api_base
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # Token æˆæœ¬è¨­å®šï¼ˆæ ¹æ“šä¸åŒ API æä¾›å•†èª¿æ•´ï¼‰
        self.cost_per_token = {
            "input": 0.0001,  # æ¯å€‹è¼¸å…¥ token çš„æˆæœ¬
            "output": 0.0002,  # æ¯å€‹è¼¸å‡º token çš„æˆæœ¬
        }

    def analyze_text(self, text: str, language: str = "auto") -> TokenAnalysis:
        """åˆ†æå–®å€‹æ–‡æœ¬çš„ token è³‡è¨Š"""

        # æª¢æ¸¬èªè¨€
        if language == "auto":
            language = self._detect_language(text)

        # ç²å– tokenization çµæœ
        token_data = self._get_tokenization(text)

        if not token_data:
            # å¦‚æœ API ä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡å–®ä¼°ç®—
            token_count = self._estimate_tokens(text)
            tokens = text.split()
            token_ids = list(range(len(tokens)))
        else:
            token_count = len(token_data.get("tokens", []))
            tokens = token_data.get("tokens", [])
            token_ids = token_data.get("token_ids", [])

        # è¨ˆç®—æ•ˆç‡æŒ‡æ¨™
        char_count = len(text)
        efficiency_ratio = char_count / max(token_count, 1)
        estimated_cost = token_count * self.cost_per_token["input"]
        complexity_score = self._calculate_complexity(text)

        return TokenAnalysis(
            text=text,
            token_count=token_count,
            tokens=tokens,
            token_ids=token_ids,
            char_count=char_count,
            efficiency_ratio=efficiency_ratio,
            estimated_cost=estimated_cost,
            language=language,
            complexity_score=complexity_score,
        )

    def _get_tokenization(self, text: str) -> Optional[Dict]:
        """å¾ TGW API ç²å– tokenization çµæœ"""
        try:
            response = requests.post(
                f"{self.api_base}/v1/internal/tokenize",
                json={"text": text},
                headers={"Content-Type": "application/json"},
                timeout=10,
            )

            if response.status_code == 200:
                return response.json()
            else:
                print(f"âš ï¸ Tokenization API å›æ‡‰éŒ¯èª¤: {response.status_code}")
                return None

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•é€£æ¥ tokenization API: {e}")
            return None

    def _estimate_tokens(self, text: str) -> int:
        """ç°¡å–®çš„ token æ•¸é‡ä¼°ç®—"""
        # åŸºæ–¼ç¶“é©—çš„ä¼°ç®—å…¬å¼
        chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
        english_words = len(re.findall(r"[a-zA-Z]+", text))
        numbers = len(re.findall(r"\d+", text))
        punctuation = len(re.findall(r"[^\w\s]", text))

        # ä¸­æ–‡å­—ç¬¦é€šå¸¸ 1-2 å€‹ tokenï¼Œè‹±æ–‡è©å½™å¹³å‡ 1.3 å€‹ token
        estimated = (
            chinese_chars * 1.5
            + english_words * 1.3
            + numbers * 0.8
            + punctuation * 0.5
        )

        return max(int(estimated), 1)

    def _detect_language(self, text: str) -> str:
        """ç°¡å–®çš„èªè¨€æª¢æ¸¬"""
        chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
        english_chars = len(re.findall(r"[a-zA-Z]", text))

        total_chars = len(re.sub(r"\s", "", text))

        if total_chars == 0:
            return "unknown"

        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars

        if chinese_ratio > 0.3:
            return "chinese"
        elif english_ratio > 0.5:
            return "english"
        else:
            return "mixed"

    def _calculate_complexity(self, text: str) -> float:
        """è¨ˆç®—æ–‡æœ¬è¤‡é›œåº¦åˆ†æ•¸"""
        # åŸºæ–¼å¤šå€‹æŒ‡æ¨™çš„è¤‡é›œåº¦è¨ˆç®—
        unique_chars = len(set(text))
        total_chars = len(text)
        avg_word_length = (
            np.mean([len(word) for word in text.split()]) if text.split() else 0
        )

        # æ¨™é»ç¬¦è™Ÿå¯†åº¦
        punctuation_density = len(re.findall(r"[^\w\s]", text)) / max(total_chars, 1)

        # è¤‡é›œåº¦åˆ†æ•¸ (0-1 ä¹‹é–“)
        complexity = min(
            1.0,
            (unique_chars / max(total_chars, 1)) * 2
            + (avg_word_length / 10)
            + punctuation_density,
        )

        return round(complexity, 3)

    def compare_prompts(
        self, prompts: List[str], names: List[str] = None
    ) -> pd.DataFrame:
        """æ¯”è¼ƒå¤šå€‹æç¤ºè©çš„ token æ•ˆç‡"""

        if names is None:
            names = [f"Prompt_{i+1}" for i in range(len(prompts))]

        results = []

        for i, prompt in enumerate(prompts):
            analysis = self.analyze_text(prompt)

            results.append(
                {
                    "Name": names[i],
                    "Token_Count": analysis.token_count,
                    "Char_Count": analysis.char_count,
                    "Efficiency_Ratio": analysis.efficiency_ratio,
                    "Estimated_Cost": analysis.estimated_cost,
                    "Language": analysis.language,
                    "Complexity": analysis.complexity_score,
                }
            )

        df = pd.DataFrame(results)

        # å„²å­˜æ¯”è¼ƒçµæœ
        timestamp = int(time.time())
        filepath = self.results_dir / f"prompt_comparison_{timestamp}.csv"
        df.to_csv(filepath, index=False, encoding="utf-8-sig")

        print(f"ğŸ“Š æ¯”è¼ƒçµæœå·²å„²å­˜: {filepath}")

        return df

    def optimize_prompt_length(
        self, prompt: str, target_reduction: float = 0.2
    ) -> Dict:
        """æœ€ä½³åŒ–æç¤ºè©é•·åº¦"""

        original_analysis = self.analyze_text(prompt)
        target_tokens = int(original_analysis.token_count * (1 - target_reduction))

        print(
            f"ğŸ¯ ç›®æ¨™ï¼šå°‡ token æ•¸å¾ {original_analysis.token_count} æ¸›å°‘åˆ° {target_tokens}"
        )

        # æœ€ä½³åŒ–ç­–ç•¥
        optimizations = []

        # 1. ç§»é™¤å†—é¤˜è©å½™
        opt1 = self._remove_redundant_words(prompt)
        if opt1 != prompt:
            opt1_analysis = self.analyze_text(opt1)
            optimizations.append(
                {
                    "strategy": "ç§»é™¤å†—é¤˜è©å½™",
                    "text": opt1,
                    "analysis": opt1_analysis,
                    "reduction": original_analysis.token_count
                    - opt1_analysis.token_count,
                }
            )

        # 2. ç°¡åŒ–å¥å¼çµæ§‹
        opt2 = self._simplify_structure(prompt)
        if opt2 != prompt:
            opt2_analysis = self.analyze_text(opt2)
            optimizations.append(
                {
                    "strategy": "ç°¡åŒ–å¥å¼çµæ§‹",
                    "text": opt2,
                    "analysis": opt2_analysis,
                    "reduction": original_analysis.token_count
                    - opt2_analysis.token_count,
                }
            )

        # 3. ä½¿ç”¨æ›´ç²¾ç°¡çš„è¡¨é”
        opt3 = self._use_concise_expressions(prompt)
        if opt3 != prompt:
            opt3_analysis = self.analyze_text(opt3)
            optimizations.append(
                {
                    "strategy": "ä½¿ç”¨ç²¾ç°¡è¡¨é”",
                    "text": opt3,
                    "analysis": opt3_analysis,
                    "reduction": original_analysis.token_count
                    - opt3_analysis.token_count,
                }
            )

        # é¸æ“‡æœ€ä½³æœ€ä½³åŒ–
        best_optimization = (
            max(optimizations, key=lambda x: x["reduction"]) if optimizations else None
        )

        return {
            "original": {"text": prompt, "analysis": original_analysis},
            "target_tokens": target_tokens,
            "optimizations": optimizations,
            "best_optimization": best_optimization,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

    def _remove_redundant_words(self, text: str) -> str:
        """ç§»é™¤å†—é¤˜è©å½™"""
        # ç§»é™¤å¸¸è¦‹çš„å†—é¤˜è¡¨é”
        redundant_patterns = [
            (r"\b(è«‹æ‚¨|è«‹ä½ |éº»ç…©æ‚¨|éº»ç…©ä½ |å¯ä»¥è«‹æ‚¨|å¯ä»¥è«‹ä½ )\b", "è«‹"),
            (r"\b(èƒ½å¤ |èƒ½å¤ å¹«æˆ‘|èƒ½å¤ å”åŠ©|èƒ½å¤ å¹«å¿™)\b", "èƒ½"),
            (r"\b(éå¸¸çš„|ç‰¹åˆ¥çš„|ç›¸ç•¶çš„|ååˆ†çš„)\b", "å¾ˆ"),
            (r"\b(é€²è¡Œ|å¯¦æ–½|åŸ·è¡Œ|é–‹å±•)\b", "åš"),
            (r"\b(æä¾›çµ¦|çµ¦äºˆ|ä¾›æ‡‰)\b", "çµ¦"),
            (r"\b(é—œæ–¼|æœ‰é—œæ–¼|é‡å°)\b", "å°æ–¼"),
            (r"\s+", " "),  # å¤šå€‹ç©ºæ ¼åˆä½µç‚ºä¸€å€‹
        ]

        optimized = text
        for pattern, replacement in redundant_patterns:
            optimized = re.sub(pattern, replacement, optimized)

        return optimized.strip()

    def _simplify_structure(self, text: str) -> str:
        """ç°¡åŒ–å¥å¼çµæ§‹"""
        # å°‡è¤‡é›œå¥å¼è½‰æ›ç‚ºç°¡å–®å¥å¼
        simplifications = [
            # ç§»é™¤éå¤šçš„ç¦®è²Œç”¨èª
            (r"ä¸å¥½æ„æ€ï¼Œ?", ""),
            (r"å¦‚æœå¯ä»¥çš„è©±ï¼Œ?", ""),
            (r"åœ¨æ‚¨æ–¹ä¾¿çš„æ™‚å€™ï¼Œ?", ""),
            # ç°¡åŒ–æ¢ä»¶å¥
            (r"å¦‚æœæ‚¨èƒ½å¤ ?(.+?)çš„è©±", r"\1"),
            (r"å‡å¦‚(.+?)çš„æƒ…æ³ä¸‹", r"è‹¥\1"),
            # ç°¡åŒ–æ™‚é–“è¡¨é”
            (r"åœ¨(.+?)çš„æ™‚å€™", r"\1æ™‚"),
            (r"ç•¶(.+?)çš„æ™‚å€™", r"\1æ™‚"),
        ]

        simplified = text
        for pattern, replacement in simplifications:
            simplified = re.sub(pattern, replacement, simplified)

        return simplified.strip()

    def _use_concise_expressions(self, text: str) -> str:
        """ä½¿ç”¨æ›´ç²¾ç°¡çš„è¡¨é”"""
        concise_replacements = [
            ("è©³ç´°çš„èªªæ˜", "èªªæ˜"),
            ("å®Œæ•´çš„å ±å‘Š", "å ±å‘Š"),
            ("å…·é«”çš„ç¯„ä¾‹", "ç¯„ä¾‹"),
            ("ç›¸é—œçš„è³‡è¨Š", "è³‡è¨Š"),
            ("é‡è¦çš„äº‹é …", "è¦é»"),
            ("ä¸»è¦çš„å…§å®¹", "å…§å®¹"),
            ("åŸºæœ¬çš„æ¦‚å¿µ", "æ¦‚å¿µ"),
            ("ä¸€èˆ¬çš„æƒ…æ³", "é€šå¸¸"),
            ("ç‰¹æ®Šçš„ç‹€æ³", "ç‰¹ä¾‹"),
            ("å¤§è‡´çš„æ–¹å‘", "æ–¹å‘"),
        ]

        concise = text
        for verbose, simple in concise_replacements:
            concise = concise.replace(verbose, simple)

        return concise

    def batch_analyze(self, texts: List[str], names: List[str] = None) -> Dict:
        """æ‰¹æ¬¡åˆ†æå¤šå€‹æ–‡æœ¬"""

        if names is None:
            names = [f"Text_{i+1}" for i in range(len(texts))]

        print(f"ğŸ”„ é–‹å§‹æ‰¹æ¬¡åˆ†æ {len(texts)} å€‹æ–‡æœ¬...")

        results = {
            "analyses": [],
            "summary": {},
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

        total_tokens = 0
        total_chars = 0
        languages = {}

        for i, text in enumerate(texts):
            print(f"  åˆ†æä¸­... {i+1}/{len(texts)}")

            analysis = self.analyze_text(text)

            results["analyses"].append(
                {"name": names[i], "analysis": analysis.__dict__}
            )

            total_tokens += analysis.token_count
            total_chars += analysis.char_count

            # çµ±è¨ˆèªè¨€åˆ†ä½ˆ
            lang = analysis.language
            languages[lang] = languages.get(lang, 0) + 1

        # ç”Ÿæˆæ‘˜è¦çµ±è¨ˆ
        results["summary"] = {
            "total_texts": len(texts),
            "total_tokens": total_tokens,
            "total_chars": total_chars,
            "avg_tokens_per_text": total_tokens / len(texts),
            "avg_chars_per_text": total_chars / len(texts),
            "avg_efficiency_ratio": total_chars / total_tokens,
            "language_distribution": languages,
            "estimated_total_cost": total_tokens * self.cost_per_token["input"],
        }

        # å„²å­˜çµæœ
        self._save_batch_results(results)

        print("âœ… æ‰¹æ¬¡åˆ†æå®Œæˆ")
        return results

    def _save_batch_results(self, results: Dict):
        """å„²å­˜æ‰¹æ¬¡åˆ†æçµæœ"""
        timestamp = int(time.time())
        filepath = self.results_dir / f"batch_analysis_{timestamp}.json"

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“ æ‰¹æ¬¡çµæœå·²å„²å­˜: {filepath}")

    def visualize_comparison(
        self, comparison_df: pd.DataFrame, save_plot: bool = True
    ) -> None:
        """è¦–è¦ºåŒ–æ¯”è¼ƒçµæœ"""

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle("Token Analysis Comparison", fontsize=16)

        # 1. Token æ•¸é‡æ¯”è¼ƒ
        axes[0, 0].bar(comparison_df["Name"], comparison_df["Token_Count"])
        axes[0, 0].set_title("Token Count Comparison")
        axes[0, 0].set_ylabel("Token Count")
        axes[0, 0].tick_params(axis="x", rotation=45)

        # 2. æ•ˆç‡æ¯”è¼ƒ
        axes[0, 1].bar(comparison_df["Name"], comparison_df["Efficiency_Ratio"])
        axes[0, 1].set_title("Efficiency Ratio (Chars/Token)")
        axes[0, 1].set_ylabel("Efficiency Ratio")
        axes[0, 1].tick_params(axis="x", rotation=45)

        # 3. æˆæœ¬æ¯”è¼ƒ
        axes[1, 0].bar(comparison_df["Name"], comparison_df["Estimated_Cost"])
        axes[1, 0].set_title("Estimated Cost Comparison")
        axes[1, 0].set_ylabel("Cost ($)")
        axes[1, 0].tick_params(axis="x", rotation=45)

        # 4. è¤‡é›œåº¦æ¯”è¼ƒ
        axes[1, 1].bar(comparison_df["Name"], comparison_df["Complexity"])
        axes[1, 1].set_title("Complexity Score")
        axes[1, 1].set_ylabel("Complexity (0-1)")
        axes[1, 1].tick_params(axis="x", rotation=45)

        plt.tight_layout()

        if save_plot:
            timestamp = int(time.time())
            plot_path = self.results_dir / f"comparison_plot_{timestamp}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            print(f"ğŸ“ˆ åœ–è¡¨å·²å„²å­˜: {plot_path}")

        plt.show()

    def generate_optimization_report(
        self, optimization_result: Dict, save_report: bool = True
    ) -> str:
        """ç”Ÿæˆæœ€ä½³åŒ–å ±å‘Š"""

        original = optimization_result["original"]
        best_opt = optimization_result["best_optimization"]

        report = f"""
# Token æœ€ä½³åŒ–å ±å‘Š

## åŸå§‹æç¤ºè©åˆ†æ
- **Token æ•¸é‡**: {original['analysis'].token_count}
- **å­—ç¬¦æ•¸é‡**: {original['analysis'].char_count}
- **æ•ˆç‡æ¯”ç‡**: {original['analysis'].efficiency_ratio:.2f}
- **é ä¼°æˆæœ¬**: ${original['analysis'].estimated_cost:.4f}
- **èªè¨€**: {original['analysis'].language}
- **è¤‡é›œåº¦**: {original['analysis'].complexity_score}

## æœ€ä½³åŒ–ç›®æ¨™
- **ç›®æ¨™ Token æ•¸**: {optimization_result['target_tokens']}
- **ç›®æ¨™æ¸›å°‘é‡**: {original['analysis'].token_count - optimization_result['target_tokens']} tokens

## æœ€ä½³åŒ–ç­–ç•¥çµæœ
"""

        if best_opt:
            reduction_percentage = (
                best_opt["reduction"] / original["analysis"].token_count
            ) * 100
            cost_saving = (
                original["analysis"].estimated_cost
                - best_opt["analysis"].estimated_cost
            )

            report += f"""
### æ¨è–¦ç­–ç•¥: {best_opt['strategy']}

**æœ€ä½³åŒ–å‰:**
```
{original['text']}
```

**æœ€ä½³åŒ–å¾Œ:**
```
{best_opt['text']}
```

**æ”¹å–„æ•ˆæœ:**
- Token æ¸›å°‘: {best_opt['reduction']} ({reduction_percentage:.1f}%)
- æˆæœ¬ç¯€çœ: ${cost_saving:.4f} ({(cost_saving/original['analysis'].estimated_cost)*100:.1f}%)
- æ–°æ•ˆç‡æ¯”ç‡: {best_opt['analysis'].efficiency_ratio:.2f}
"""

        # æ‰€æœ‰ç­–ç•¥æ¯”è¼ƒ
        report += "\n## æ‰€æœ‰ç­–ç•¥æ¯”è¼ƒ\n\n"
        for opt in optimization_result["optimizations"]:
            reduction_pct = (opt["reduction"] / original["analysis"].token_count) * 100
            report += f"- **{opt['strategy']}**: æ¸›å°‘ {opt['reduction']} tokens ({reduction_pct:.1f}%)\n"

        if save_report:
            timestamp = int(time.time())
            report_path = self.results_dir / f"optimization_report_{timestamp}.md"

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report)

            print(f"ğŸ“„ æœ€ä½³åŒ–å ±å‘Šå·²å„²å­˜: {report_path}")

        return report


def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ” TGW Token Analyzer & Optimizer")
    print("=" * 50)

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = TGWTokenAnalyzer()

    # ç¯„ä¾‹åˆ†æ
    print("ğŸ“Š åŸ·è¡Œç¯„ä¾‹åˆ†æ...")

    sample_prompts = [
        "è«‹æ‚¨å”åŠ©æˆ‘æ’°å¯«ä¸€ä»½é—œæ–¼äººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•çš„è©³ç´°å ±å‘Šï¼Œå…§å®¹éœ€è¦åŒ…å«ç•¶å‰çš„æŠ€è¡“è¶¨å‹¢ã€ä¸»è¦æ‡‰ç”¨é ˜åŸŸä»¥åŠæœªä¾†çš„ç™¼å±•æ–¹å‘ã€‚",
        "å¯«ä¸€ä»½ AI æŠ€è¡“å ±å‘Šï¼ŒåŒ…å«æŠ€è¡“è¶¨å‹¢ã€æ‡‰ç”¨é ˜åŸŸã€ç™¼å±•æ–¹å‘ã€‚",
        "Generate an AI technology report covering trends, applications, and future directions.",
    ]

    sample_names = ["å†—é•·ç‰ˆæœ¬", "ç²¾ç°¡ç‰ˆæœ¬", "è‹±æ–‡ç‰ˆæœ¬"]

    # æ¯”è¼ƒåˆ†æ
    comparison_df = analyzer.compare_prompts(sample_prompts, sample_names)
    print("\nğŸ“‹ æç¤ºè©æ¯”è¼ƒçµæœ:")
    print(comparison_df.to_string(index=False))

    # æœ€ä½³åŒ–ç¤ºç¯„
    print("\nğŸ”§ åŸ·è¡Œæœ€ä½³åŒ–ç¤ºç¯„...")
    optimization_result = analyzer.optimize_prompt_length(
        sample_prompts[0], target_reduction=0.3
    )

    # ç”Ÿæˆå ±å‘Š
    report = analyzer.generate_optimization_report(optimization_result)
    print("\nğŸ“„ æœ€ä½³åŒ–å ±å‘Š:")
    print(report[:500] + "..." if len(report) > 500 else report)

    print(f"\nâœ… åˆ†æå®Œæˆï¼çµæœå„²å­˜åœ¨: {analyzer.results_dir}")


if __name__ == "__main__":
    main()
