#!/usr/bin/env python3
"""
TGW Stage 3 å®Œæ•´å·¥å…·åŒ…
æ•´åˆæ‰€æœ‰ Stage 3 åŠŸèƒ½çš„ä¸»è¦å·¥å…·é¡åˆ¥
"""

import os
import sys
import json
import time
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import requests
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import yaml

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TGWToolkit:
    """TGW Stage 3 å®Œæ•´å·¥å…·åŒ…"""

    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–å·¥å…·åŒ…"""
        self.base_dir = Path(__file__).parent.parent
        self.config = self._load_config(config_path)
        self.api_base = self.config.get('tgw_api', {}).get('base_url', 'http://localhost:5000')
        self.session = requests.Session()

        # å»ºç«‹çµæœç›®éŒ„
        self.results_dir = self.base_dir / 'results'
        self.results_dir.mkdir(exist_ok=True)

        logger.info(f"TGW Toolkit åˆå§‹åŒ–å®Œæˆï¼ŒAPI: {self.api_base}")

    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        if config_path is None:
            config_path = self.base_dir / 'configs' / 'advanced_config.yaml'

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.warning(f"é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é è¨­é…ç½®")
            return self._get_default_config()

    def _get_default_config(self) -> Dict:
        """å–å¾—é è¨­é…ç½®"""
        return {
            'tgw_api': {'base_url': 'http://localhost:5000'},
            'token_analysis': {'efficiency': {'good_ratio': 2.5}},
            'batch_generation': {'concurrency': {'max_concurrent_requests': 3}},
            'quality_assessment': {'metrics': {'readability_weight': 0.25}}
        }

    def health_check(self) -> bool:
        """æª¢æŸ¥ TGW API å¥åº·ç‹€æ…‹"""
        try:
            response = self.session.get(f"{self.api_base}/v1/models", timeout=10)
            if response.status_code == 200:
                logger.info("TGW API å¥åº·ç‹€æ…‹è‰¯å¥½")
                return True
            else:
                logger.error(f"TGW API å›æ‡‰ç•°å¸¸: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"TGW API é€£æ¥å¤±æ•—: {e}")
            return False

    def analyze_prompt_efficiency(self, prompt: str) -> Dict[str, Any]:
        """åˆ†ææç¤ºè©æ•ˆç‡"""
        try:
            # å–å¾— tokenization çµæœ
            response = self.session.post(
                f"{self.api_base}/v1/internal/tokenize",
                json={"text": prompt},
                timeout=30
            )

            if response.status_code != 200:
                return {"error": f"API éŒ¯èª¤: {response.status_code}"}

            data = response.json()
            tokens = data.get("tokens", [])

            # è¨ˆç®—æ•ˆç‡æŒ‡æ¨™
            char_count = len(prompt)
            token_count = len(tokens)
            efficiency_ratio = char_count / max(token_count, 1)

            # è¤‡é›œåº¦åˆ†æ
            complexity = self._calculate_complexity(prompt)

            # èªè¨€æª¢æ¸¬
            language = self._detect_language(prompt)

            return {
                "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
                "char_count": char_count,
                "token_count": token_count,
                "tokens": tokens[:10],  # åªè¿”å›å‰10å€‹tokens
                "efficiency_ratio": round(efficiency_ratio, 3),
                "complexity_score": complexity,
                "language": language,
                "quality_assessment": self._assess_prompt_quality(prompt, efficiency_ratio),
                "recommendations": self._generate_efficiency_recommendations(efficiency_ratio, complexity),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æç¤ºè©åˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}

    def _calculate_complexity(self, text: str) -> float:
        """è¨ˆç®—æ–‡æœ¬è¤‡é›œåº¦"""
        if not text:
            return 0.0

        # åŸºæœ¬è¤‡é›œåº¦æŒ‡æ¨™
        unique_chars = len(set(text))
        total_chars = len(text)
        words = text.split()
        avg_word_length = sum(len(word) for word in words) / max(len(words), 1)

        # æ¨™é»ç¬¦è™Ÿå¯†åº¦
        import re
        punctuation_count = len(re.findall(r'[^\w\s]', text))
        punctuation_density = punctuation_count / max(total_chars, 1)

        # è¤‡é›œåº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰
        complexity = min(1.0,
            (unique_chars / max(total_chars, 1)) * 2 +
            (avg_word_length / 15) * 0.5 +
            punctuation_density * 2
        )

        return round(complexity, 3)

    def _detect_language(self, text: str) -> str:
        """ç°¡å–®èªè¨€æª¢æ¸¬"""
        import re

        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(re.sub(r'\s', '', text))

        if total_chars == 0:
            return "unknown"

        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars

        if chinese_ratio > 0.3:
            return "chinese"
        elif english_ratio > 0.5:
            return "english"
        else:
            return "mixed"

    def _assess_prompt_quality(self, prompt: str, efficiency_ratio: float) -> Dict[str, Any]:
        """è©•ä¼°æç¤ºè©å“è³ª"""
        quality_scores = {}

        # æ•ˆç‡è©•åˆ†
        if efficiency_ratio >= 3.0:
            quality_scores["efficiency"] = {"score": 0.9, "level": "excellent"}
        elif efficiency_ratio >= 2.5:
            quality_scores["efficiency"] = {"score": 0.7, "level": "good"}
        elif efficiency_ratio >= 2.0:
            quality_scores["efficiency"] = {"score": 0.5, "level": "acceptable"}
        else:
            quality_scores["efficiency"] = {"score": 0.3, "level": "poor"}

        # æ¸…æ™°åº¦è©•åˆ†ï¼ˆåŸºæ–¼çµæ§‹åŒ–ç¨‹åº¦ï¼‰
        structure_indicators = ["ï¼š", ":", "1.", "2.", "3.", "â€¢", "-", "è¦æ±‚", "æ ¼å¼", "åŒ…å«"]
        structure_score = sum(1 for indicator in structure_indicators if indicator in prompt)
        clarity_score = min(1.0, structure_score / 5)

        quality_scores["clarity"] = {
            "score": clarity_score,
            "level": "excellent" if clarity_score >= 0.8 else "good" if clarity_score >= 0.6 else "acceptable"
        }

        # å…·é«”æ€§è©•åˆ†ï¼ˆåŸºæ–¼å…·é«”è¦æ±‚ï¼‰
        specificity_indicators = ["å­—æ•¸", "æ ¼å¼", "åŒ…å«", "è¦æ±‚", "ç¯„ä¾‹", "æ­¥é©Ÿ"]
        specificity_score = sum(1 for indicator in specificity_indicators if indicator in prompt)
        specificity_normalized = min(1.0, specificity_score / 4)

        quality_scores["specificity"] = {
            "score": specificity_normalized,
            "level": "excellent" if specificity_normalized >= 0.8 else "good" if specificity_normalized >= 0.6 else "acceptable"
        }

        # è¨ˆç®—ç¸½é«”å“è³ªåˆ†æ•¸
        overall_score = (
            quality_scores["efficiency"]["score"] * 0.4 +
            quality_scores["clarity"]["score"] * 0.3 +
            quality_scores["specificity"]["score"] * 0.3
        )

        quality_scores["overall"] = {
            "score": round(overall_score, 3),
            "level": "excellent" if overall_score >= 0.8 else "good" if overall_score >= 0.6 else "needs_improvement"
        }

        return quality_scores

    def _generate_efficiency_recommendations(self, efficiency_ratio: float, complexity: float) -> List[str]:
        """ç”Ÿæˆæ•ˆç‡æ”¹é€²å»ºè­°"""
        recommendations = []

        if efficiency_ratio < 2.0:
            recommendations.append("è€ƒæ…®ç§»é™¤å†—é¤˜è©å½™ä»¥æé«˜ token æ•ˆç‡")
            recommendations.append("ä½¿ç”¨æ›´ç²¾ç°¡çš„è¡¨é”æ–¹å¼")

        if complexity > 0.7:
            recommendations.append("ç°¡åŒ–å¥å¼çµæ§‹ä»¥æé«˜å¯è®€æ€§")
            recommendations.append("æ¸›å°‘è¤‡é›œçš„æ¨™é»ç¬¦è™Ÿä½¿ç”¨")

        if efficiency_ratio < 2.5 and complexity < 0.3:
            recommendations.append("åœ¨ä¿æŒç°¡æ½”çš„åŒæ™‚ï¼Œå¯ä»¥å¢åŠ æ›´å…·é«”çš„è¦æ±‚")

        if not recommendations:
            recommendations.append("æç¤ºè©æ•ˆç‡è‰¯å¥½ï¼Œå»ºè­°ä¿æŒç¾æœ‰é¢¨æ ¼")

        return recommendations

    def batch_generate_with_analysis(self,
                                   prompts: List[str],
                                   parameters: Dict[str, Any] = None,
                                   analyze_quality: bool = True) -> Dict[str, Any]:
        """æ‰¹æ¬¡ç”Ÿæˆä¸¦é€²è¡Œå“è³ªåˆ†æ"""

        if parameters is None:
            parameters = self.config.get('batch_generation', {}).get('parameter_presets', {}).get('balanced', {})

        results = {
            "tasks": [],
            "summary": {},
            "quality_analysis": {},
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"é–‹å§‹æ‰¹æ¬¡ç”Ÿæˆ {len(prompts)} å€‹æç¤ºè©...")

        for i, prompt in enumerate(prompts):
            logger.info(f"è™•ç†æç¤ºè© {i+1}/{len(prompts)}")

            # ç”Ÿæˆå…§å®¹
            generation_result = self._single_generate(prompt, parameters)

            # åˆ†æå“è³ªï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            quality_metrics = {}
            if analyze_quality and generation_result.get("success", False):
                quality_metrics = self._analyze_generation_quality(
                    prompt,
                    generation_result.get("generated_text", "")
                )

            task_result = {
                "task_id": f"task_{i+1:03d}",
                "prompt": prompt,
                "generation_result": generation_result,
                "quality_metrics": quality_metrics,
                "timestamp": datetime.now().isoformat()
            }

            results["tasks"].append(task_result)

        # ç”Ÿæˆæ‘˜è¦çµ±è¨ˆ
        results["summary"] = self._generate_batch_summary(results["tasks"])

        # ç”Ÿæˆå“è³ªåˆ†æå ±å‘Š
        if analyze_quality:
            results["quality_analysis"] = self._generate_quality_analysis(results["tasks"])

        # å„²å­˜çµæœ
        self._save_batch_results(results)

        logger.info("æ‰¹æ¬¡ç”Ÿæˆå®Œæˆ")
        return results

    def _single_generate(self, prompt: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """å–®ä¸€ç”Ÿæˆè«‹æ±‚"""
        try:
            start_time = time.time()

            response = self.session.post(
                f"{self.api_base}/v1/completions",
                json={
                    "prompt": prompt,
                    **parameters
                },
                timeout=60
            )

            generation_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("choices", [{}])[0].get("text", "")

                return {
                    "success": True,
                    "generated_text": generated_text,
                    "generation_time": round(generation_time, 2),
                    "token_usage": data.get("usage", {}),
                    "parameters": parameters
                }
            else:
                return {
                    "success": False,
                    "error": f"API éŒ¯èª¤: {response.status_code}",
                    "generation_time": round(generation_time, 2)
                }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "generation_time": 0
            }

    def _analyze_generation_quality(self, prompt: str, generated_text: str) -> Dict[str, Any]:
        """åˆ†æç”Ÿæˆå…§å®¹å“è³ª"""
        if not generated_text:
            return {"error": "æ²’æœ‰ç”Ÿæˆå…§å®¹å¯åˆ†æ"}

        # åŸºæœ¬çµ±è¨ˆ
        char_count = len(generated_text)
        word_count = len(generated_text.split())

        # è¨ˆç®—å“è³ªæŒ‡æ¨™
        readability = self._calculate_readability(generated_text)
        coherence = self._calculate_coherence(generated_text)
        completeness = self._calculate_completeness(generated_text, prompt)
        relevance = self._calculate_relevance(generated_text, prompt)

        # æ•´é«”å“è³ªåˆ†æ•¸
        weights = self.config.get('quality_assessment', {}).get('metrics', {})
        overall_quality = (
            readability * weights.get('readability_weight', 0.25) +
            coherence * weights.get('coherence_weight', 0.25) +
            completeness * weights.get('completeness_weight', 0.25) +
            relevance * weights.get('relevance_weight', 0.25)
        )

        return {
            "basic_stats": {
                "char_count": char_count,
                "word_count": word_count
            },
            "quality_scores": {
                "readability": round(readability, 3),
                "coherence": round(coherence, 3),
                "completeness": round(completeness, 3),
                "relevance": round(relevance, 3),
                "overall": round(overall_quality, 3)
            }
        }

    def _calculate_readability(self, text: str) -> float:
        """è¨ˆç®—å¯è®€æ€§åˆ†æ•¸"""
        import re

        if not text.strip():
            return 0.0

        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            return 0.0

        # å¹³å‡å¥å­é•·åº¦
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)

        # å¯è®€æ€§åˆ†æ•¸ï¼ˆåŸºæ–¼å¥å­é•·åº¦ï¼Œç†æƒ³é•·åº¦ç‚º15å­—å·¦å³ï¼‰
        readability = max(0, min(1, 1 - abs(avg_sentence_length - 15) / 30))

        return readability

    def _calculate_coherence(self, text: str) -> float:
        """è¨ˆç®—é€£è²«æ€§åˆ†æ•¸"""
        if not text.strip():
            return 0.0

        # æª¢æŸ¥é€£æ¥è©ä½¿ç”¨
        connectors = self.config.get('quality_assessment', {}).get('coherence', {}).get('connector_words', [])
        connector_count = sum(1 for connector in connectors if connector in text)

        # åŸºæ–¼æ–‡æœ¬é•·åº¦çš„æœŸæœ›é€£æ¥è©æ•¸é‡
        words = text.split()
        expected_connectors = len(words) / 50  # æ¯50å€‹è©æœŸæœ›æœ‰1å€‹é€£æ¥è©

        coherence_score = min(1.0, connector_count / max(expected_connectors, 1))

        return coherence_score

    def _calculate_completeness(self, text: str, prompt: str) -> float:
        """è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸"""
        if not text.strip():
            return 0.0

        # é•·åº¦ç›¸å°æ€§æª¢æŸ¥
        min_ratio = self.config.get('quality_assessment', {}).get('completeness', {}).get('min_length_ratio', 2.0)
        length_ratio = len(text) / max(len(prompt), 1)
        length_score = min(1.0, length_ratio / min_ratio)

        # çµå°¾å®Œæ•´æ€§æª¢æŸ¥
        conclusion_phrases = self.config.get('quality_assessment', {}).get('completeness', {}).get('required_conclusion_phrases', [])
        has_conclusion = any(phrase in text for phrase in conclusion_phrases)
        conclusion_score = 1.0 if has_conclusion else 0.7

        completeness = length_score * 0.7 + conclusion_score * 0.3

        return completeness

    def _calculate_relevance(self, text: str, prompt: str) -> float:
        """è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸"""
        if not text.strip() or not prompt.strip():
            return 0.0

        import re

        # æå–é—œéµè©
        prompt_words = set(re.findall(r'\b\w+\b', prompt.lower()))
        text_words = set(re.findall(r'\b\w+\b', text.lower()))

        # éæ¿¾å¸¸è¦‹è©
        common_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'èˆ‡', 'æˆ–', 'ä½†', 'é€™', 'é‚£', 'ä¸€', 'å€‹', 'a', 'an', 'the', 'is', 'are', 'and', 'or', 'but'}
        prompt_words -= common_words
        text_words -= common_words

        # è¨ˆç®—é‡ç–Šåº¦
        if prompt_words:
            overlap_ratio = len(prompt_words & text_words) / len(prompt_words)
        else:
            overlap_ratio = 0.0

        return overlap_ratio

    def _generate_batch_summary(self, tasks: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰¹æ¬¡è™•ç†æ‘˜è¦"""
        total_tasks = len(tasks)
        successful_tasks = [t for t in tasks if t["generation_result"].get("success", False)]
        failed_tasks = [t for t in tasks if not t["generation_result"].get("success", False)]

        if successful_tasks:
            avg_generation_time = sum(t["generation_result"]["generation_time"] for t in successful_tasks) / len(successful_tasks)

            # å“è³ªçµ±è¨ˆ
            quality_scores = []
            for task in successful_tasks:
                if "quality_scores" in task.get("quality_metrics", {}):
                    quality_scores.append(task["quality_metrics"]["quality_scores"]["overall"])

            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        else:
            avg_generation_time = 0
            avg_quality = 0

        return {
            "total_tasks": total_tasks,
            "successful_tasks": len(successful_tasks),
            "failed_tasks": len(failed_tasks),
            "success_rate": len(successful_tasks) / total_tasks * 100 if total_tasks > 0 else 0,
            "avg_generation_time": round(avg_generation_time, 2),
            "avg_quality_score": round(avg_quality, 3)
        }

    def _generate_quality_analysis(self, tasks: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå“è³ªåˆ†æå ±å‘Š"""
        quality_data = []

        for task in tasks:
            if "quality_scores" in task.get("quality_metrics", {}):
                scores = task["quality_metrics"]["quality_scores"]
                quality_data.append(scores)

        if not quality_data:
            return {"message": "æ²’æœ‰å¯åˆ†æçš„å“è³ªè³‡æ–™"}

        # è¨ˆç®—å„é …æŒ‡æ¨™çš„å¹³å‡å€¼å’Œåˆ†ä½ˆ
        metrics = ["readability", "coherence", "completeness", "relevance", "overall"]
        analysis = {}

        for metric in metrics:
            values = [data[metric] for data in quality_data if metric in data]
            if values:
                analysis[metric] = {
                    "mean": round(sum(values) / len(values), 3),
                    "min": round(min(values), 3),
                    "max": round(max(values), 3),
                    "count": len(values)
                }

        # å“è³ªç­‰ç´šåˆ†ä½ˆ
        overall_scores = [data["overall"] for data in quality_data if "overall" in data]
        quality_distribution = {
            "excellent": len([s for s in overall_scores if s >= 0.8]),
            "good": len([s for s in overall_scores if 0.6 <= s < 0.8]),
            "acceptable": len([s for s in overall_scores if 0.4 <= s < 0.6]),
            "poor": len([s for s in overall_scores if s < 0.4])
        }

        analysis["distribution"] = quality_distribution

        return analysis

    def _save_batch_results(self, results: Dict[str, Any]):
        """å„²å­˜æ‰¹æ¬¡çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = self.results_dir / f"batch_results_{timestamp}.json"

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        logger.info(f"æ‰¹æ¬¡çµæœå·²å„²å­˜: {filepath}")

    def visualize_quality_analysis(self, results: Dict[str, Any], save_plot: bool = True) -> None:
        """è¦–è¦ºåŒ–å“è³ªåˆ†æçµæœ"""
        if "quality_analysis" not in results:
            logger.warning("æ²’æœ‰å“è³ªåˆ†æè³‡æ–™å¯è¦–è¦ºåŒ–")
            return

        quality_analysis = results["quality_analysis"]

        if "message" in quality_analysis:
            logger.warning(quality_analysis["message"])
            return

        # è¨­å®šç¹ªåœ–é¢¨æ ¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('TGW æ‰¹æ¬¡ç”Ÿæˆå“è³ªåˆ†æå ±å‘Š', fontsize=16, fontweight='bold')

        # 1. å“è³ªæŒ‡æ¨™æ¯”è¼ƒ
        metrics = ["readability", "coherence", "completeness", "relevance"]
        means = [quality_analysis[metric]["mean"] for metric in metrics if metric in quality_analysis]

        if means:
            axes[0, 0].bar(metrics, means, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
            axes[0, 0].set_title('å„é …å“è³ªæŒ‡æ¨™å¹³å‡åˆ†æ•¸')
            axes[0, 0].set_ylabel('åˆ†æ•¸')
            axes[0, 0].set_ylim(0, 1)
            axes[0, 0].tick_params(axis='x', rotation=45)

        # 2. å“è³ªåˆ†ä½ˆ
        if "distribution" in quality_analysis:
            dist = quality_analysis["distribution"]
            labels = list(dist.keys())
            values = list(dist.values())

            colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']
            axes[0, 1].pie(values, labels=labels, colors=colors, autopct='%1.1f%%')
            axes[0, 1].set_title('å“è³ªç­‰ç´šåˆ†ä½ˆ')

        # 3. æˆåŠŸç‡å’Œå¹³å‡æ™‚é–“
        summary = results.get("summary", {})
        if summary:
            success_rate = summary.get("success_rate", 0)
            avg_time = summary.get("avg_generation_time", 0)

            axes[1, 0].bar(['æˆåŠŸç‡ (%)', 'å¹³å‡æ™‚é–“ (ç§’)'], [success_rate, avg_time * 10],
                          color=['#2ecc71', '#3498db'])
            axes[1, 0].set_title('è™•ç†æ•ˆç‡çµ±è¨ˆ')
            axes[1, 0].set_ylabel('æ•¸å€¼')

        # 4. å“è³ªåˆ†æ•¸ç¯„åœ
        if "overall" in quality_analysis:
            overall = quality_analysis["overall"]
            data_to_plot = [overall["min"], overall["mean"], overall["max"]]
            labels = ['æœ€ä½', 'å¹³å‡', 'æœ€é«˜']

            axes[1, 1].bar(labels, data_to_plot, color=['#e74c3c', '#f39c12', '#2ecc71'])
            axes[1, 1].set_title('æ•´é«”å“è³ªåˆ†æ•¸ç¯„åœ')
            axes[1, 1].set_ylabel('åˆ†æ•¸')
            axes[1, 1].set_ylim(0, 1)

        plt.tight_layout()

        if save_plot:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_path = self.results_dir / f"quality_analysis_{timestamp}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
            logger.info(f"å“è³ªåˆ†æåœ–è¡¨å·²å„²å­˜: {plot_path}")

        plt.show()

    def generate_comprehensive_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary = results.get("summary", {})
        quality_analysis = results.get("quality_analysis", {})

        report = f"""# TGW Stage 3 æ‰¹æ¬¡ç”Ÿæˆåˆ†æå ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {timestamp}
**åˆ†æç‰ˆæœ¬**: TGW Toolkit v1.0

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

- **ç¸½ä»»å‹™æ•¸**: {summary.get('total_tasks', 0)}
- **æˆåŠŸä»»å‹™**: {summary.get('successful_tasks', 0)}
- **å¤±æ•—ä»»å‹™**: {summary.get('failed_#!/bin/bash
# Stage 3 å®Œæ•´éƒ¨ç½²å’Œè¨­ç½®è…³æœ¬
# è‡ªå‹•å»ºç«‹å®Œæ•´çš„ Stage 3 ç’°å¢ƒå’Œæ‰€æœ‰å¿…è¦æª”æ¡ˆ

set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³é€€å‡º

# é¡è‰²è¨­å®š
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# å‡½æ•¸ï¼šé¡¯ç¤ºå¸¶é¡è‰²çš„è¨Šæ¯
print_message() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

print_success() { print_message $GREEN "âœ… $1"; }
print_warning() { print_message $YELLOW "âš ï¸  $1"; }
print_error() { print_message $RED "âŒ $1"; }
print_info() { print_message $CYAN "â„¹ï¸  $1"; }
print_header() { print_message $PURPLE "ğŸ¯ $1"; }

# è¨­å®šè·¯å¾‘è®Šæ•¸
PLAYBOOK_ROOT="/mnt/c/AI_LLM_projects/tgw-playbook"
STAGE3_DIR="$PLAYBOOK_ROOT/stages/stage3-default-notebook"
TGW_REPO="/mnt/c/AI_LLM_projects/text-generation-webui"

print_header "TGW Stage 3 - Default/Notebook Tabs å®Œæ•´éƒ¨ç½²"
echo "=========================================="

# æª¢æŸ¥å¿…è¦çš„å‰ç½®æ¢ä»¶
print_info "æª¢æŸ¥å‰ç½®æ¢ä»¶..."

# æª¢æŸ¥ Python
if ! command -v python3 &> /dev/null; then
    print_error "Python3 æœªå®‰è£æˆ–ä¸åœ¨ PATH ä¸­"
    exit 1
fi
print_success "Python3 ç’°å¢ƒæ­£å¸¸"

# æª¢æŸ¥ curl å’Œ jq
if ! command -v curl &> /dev/null; then
    print_warning "curl æœªå®‰è£ï¼Œå°‡å®‰è£..."
    sudo apt update && sudo apt install -y curl
fi

if ! command -v jq &> /dev/null; then
    print_warning "jq æœªå®‰è£ï¼Œå°‡å®‰è£..."
    sudo apt update && sudo apt install -y jq
fi

# æª¢æŸ¥åŸºç¤ç›®éŒ„
if [ ! -d "$PLAYBOOK_ROOT" ]; then
    print_error "Playbook æ ¹ç›®éŒ„ä¸å­˜åœ¨: $PLAYBOOK_ROOT"
    print_info "è«‹å…ˆåŸ·è¡Œ Stage 0 è¨­ç½®"
    exit 1
fi
print_success "Playbook æ ¹ç›®éŒ„å­˜åœ¨"

# å»ºç«‹ Stage 3 ç›®éŒ„çµæ§‹
print_info "å»ºç«‹ Stage 3 ç›®éŒ„çµæ§‹..."
