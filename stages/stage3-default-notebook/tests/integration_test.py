#!/usr/bin/env python3
"""
Stage 3 æ•´åˆæ¸¬è©¦å¥—ä»¶
"""

import unittest
import sys
import os
import json
import tempfile
from pathlib import Path

# æ·»åŠ å·¥å…·åŒ…è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'tools'))

from token_analyzer import AdvancedTokenAnalyzer
from batch_processor import AdvancedBatchProcessor, BatchTask
from visualization import VisualizationEngine, ReportGenerator

class TestStage3Integration(unittest.TestCase):
    """Stage 3 æ•´åˆæ¸¬è©¦"""

    def setUp(self):
        """æ¸¬è©¦å‰æº–å‚™"""
        self.token_analyzer = AdvancedTokenAnalyzer()
        self.batch_processor = AdvancedBatchProcessor(max_concurrent=1)
        self.viz_engine = VisualizationEngine()
        self.report_gen = ReportGenerator()

    def test_token_analyzer_integration(self):
        """æ¸¬è©¦ Token åˆ†æå™¨æ•´åˆ"""

        # æ¸¬è©¦ä¸­æ–‡æ–‡æœ¬
        chinese_text = "è«‹åˆ†æé€™å€‹ä¸­æ–‡æ–‡æœ¬çš„tokenä½¿ç”¨æ•ˆç‡"
        analysis = self.token_analyzer.analyze(chinese_text)

        self.assertIsInstance(analysis.token_count, int)
        self.assertIsInstance(analysis.efficiency_ratio, float)
        self.assertEqual(analysis.language, "chinese")
        self.assertGreater(analysis.complexity, 0)
        self.assertIsInstance(analysis.recommendations, list)
        self.assertGreater(len(analysis.recommendations), 0)

        # æ¸¬è©¦è‹±æ–‡æ–‡æœ¬
        english_text = "Analyze the token efficiency of this English text"
        analysis = self.token_analyzer.analyze(english_text)

        self.assertEqual(analysis.language, "english")
        self.assertGreater(analysis.efficiency_ratio, 0)

    def test_batch_processor_workflow(self):
        """æ¸¬è©¦æ‰¹æ¬¡è™•ç†å™¨å·¥ä½œæµ"""

        # å»ºç«‹æ¸¬è©¦ä»»å‹™
        test_prompts = [
            "é€™æ˜¯ç¬¬ä¸€å€‹æ¸¬è©¦æç¤ºè©",
            "é€™æ˜¯ç¬¬äºŒå€‹æ¸¬è©¦æç¤ºè©"
        ]

        tasks = self.batch_processor.create_tasks(
            prompts=test_prompts,
            parameters={"temperature": 0.5, "max_new_tokens": 100}
        )

        self.assertEqual(len(tasks), 2)
        self.assertIsInstance(tasks[0], BatchTask)
        self.assertEqual(tasks[0].parameters["temperature"], 0.5)

        # æ¸¬è©¦å“è³ªæ§åˆ¶å™¨
        quality_score = self.batch_processor.quality_controller.evaluate_quality(
            "é€™æ˜¯ä¸€å€‹å®Œæ•´çš„æ¸¬è©¦å›æ‡‰ã€‚",
            "ç”Ÿæˆä¸€å€‹æ¸¬è©¦å›æ‡‰"
        )

        self.assertGreaterEqual(quality_score, 0)
        self.assertLessEqual(quality_score, 1)

    @unittest.skipUnless(
        os.environ.get('TGW_FULL_TEST', '').lower() == 'true',
        "éœ€è¦è¨­å®š TGW_FULL_TEST=true æ‰åŸ·è¡Œå®Œæ•´æ¸¬è©¦"
    )
    def test_end_to_end_workflow(self):
        """ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æ¸¬è©¦ï¼ˆéœ€è¦ TGW é‹è¡Œï¼‰"""

        # 1. Token åˆ†æéšæ®µ
        test_prompt = "è«‹å¯«ä¸€ç¯‡é—œæ–¼æ©Ÿå™¨å­¸ç¿’çš„ç°¡çŸ­ä»‹ç´¹"
        token_analysis = self.token_analyzer.analyze(test_prompt)

        self.assertGreater(token_analysis.token_count, 0)

        # 2. æ‰¹æ¬¡è™•ç†éšæ®µ
        tasks = self.batch_processor.create_tasks([test_prompt])

        # æ³¨æ„ï¼šé€™å€‹æ¸¬è©¦éœ€è¦ TGW API é‹è¡Œ
        try:
            results = self.batch_processor.process_batch(tasks)

            self.assertIn('successful_results', results)
            self.assertIn('summary', results)

            # 3. è¦–è¦ºåŒ–æ¸¬è©¦ï¼ˆå»ºç«‹ä½†ä¸é¡¯ç¤ºï¼‰
            if results['successful_results']:
                with tempfile.TemporaryDirectory() as temp_dir:
                    dashboard_path = os.path.join(temp_dir, "test_dashboard.png")

                    # æ¸¬è©¦å„€è¡¨æ¿ç”Ÿæˆï¼ˆä¸é¡¯ç¤ºï¼‰
                    import matplotlib
                    matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯

                    self.viz_engine.create_quality_dashboard(results, dashboard_path)
                    self.assertTrue(os.path.exists(dashboard_path))

            # 4. å ±å‘Šç”Ÿæˆæ¸¬è©¦
            report = self.report_gen.generate_executive_summary(results)
            self.assertIn('TGW Stage 3', report)
            self.assertIn('åŸ·è¡Œæ‘˜è¦', report)

        except Exception as e:
            self.skipTest(f"API æ¸¬è©¦å¤±æ•—ï¼Œå¯èƒ½ TGW æœªé‹è¡Œ: {e}")

    def test_configuration_loading(self):
        """æ¸¬è©¦é…ç½®æª”æ¡ˆè¼‰å…¥"""

        config_path = Path(__file__).parent.parent / "configs" / "stage3_config.yaml"

        if config_path.exists():
            import yaml

            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

            # é©—è­‰é…ç½®çµæ§‹
            required_sections = ['api', 'batch_processing', 'quality_assessment',
                               'token_analysis', 'visualization']

            for section in required_sections:
                self.assertIn(section, config)

            # é©—è­‰ API é…ç½®
            api_config = config['api']
            self.assertIn('base_url', api_config)
            self.assertIn('timeout', api_config)

            # é©—è­‰æ‰¹æ¬¡è™•ç†é…ç½®
            batch_config = config['batch_processing']
            self.assertIn('max_concurrent', batch_config)
            self.assertIn('quality_threshold', batch_config)

    def test_error_handling(self):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†æ©Ÿåˆ¶"""

        # æ¸¬è©¦ç„¡æ•ˆæ–‡æœ¬
        empty_analysis = self.token_analyzer.analyze("")
        self.assertEqual(empty_analysis.token_count, 1)  # æœ€å°å€¼
        self.assertEqual(empty_analysis.language, "unknown")

        # æ¸¬è©¦ç„¡æ•ˆä»»å‹™
        invalid_tasks = []
        results = self.batch_processor.process_batch(invalid_tasks)

        self.assertEqual(results['summary']['total_tasks'], 0)
        self.assertEqual(results['summary']['successful_count'], 0)

    def test_data_persistence(self):
        """æ¸¬è©¦è³‡æ–™æŒä¹…åŒ–"""

        # å»ºç«‹æ¸¬è©¦çµæœ
        test_results = {
            "successful_results": [{
                "task_id": "test_001",
                "success": True,
                "generated_text": "æ¸¬è©¦ç”Ÿæˆå…§å®¹",
                "generation_time": 2.5,
                "quality_score": 0.75
            }],
            "failed_results": [],
            "summary": {
                "total_tasks": 1,
                "successful_count": 1,
                "success_rate": 100.0,
                "average_quality_score": 0.75
            },
            "timestamp": "2024-01-15 10:00:00"
        }

        # æ¸¬è©¦çµæœå„²å­˜
        with tempfile.TemporaryDirectory() as temp_dir:
            output_path = os.path.join(temp_dir, "test_results.json")

            self.batch_processor.save_results(test_results, output_path)
            self.assertTrue(os.path.exists(output_path))

            # é©—è­‰è¼‰å…¥çš„è³‡æ–™
            with open(output_path, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)

            self.assertEqual(loaded_data['summary']['total_tasks'], 1)
            self.assertEqual(loaded_data['summary']['success_rate'], 100.0)

class TestStage3Performance(unittest.TestCase):
    """Stage 3 æ•ˆèƒ½æ¸¬è©¦"""

    def setUp(self):
        self.token_analyzer = AdvancedTokenAnalyzer()

    def test_token_analysis_performance(self):
        """æ¸¬è©¦ Token åˆ†ææ•ˆèƒ½"""

        import time

        # æ¸¬è©¦å¤§å‹æ–‡æœ¬åˆ†ææ•ˆèƒ½
        large_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ã€‚" * 100

        start_time = time.time()
        analysis = self.token_analyzer.analyze(large_text)
        analysis_time = time.time() - start_time

        # æ‡‰è©²åœ¨ 1 ç§’å…§å®Œæˆ
        self.assertLess(analysis_time, 1.0)
        self.assertGreater(analysis.token_count, 0)

    def test_batch_task_creation_performance(self):
        """æ¸¬è©¦æ‰¹æ¬¡ä»»å‹™å»ºç«‹æ•ˆèƒ½"""

        import time

        batch_processor = AdvancedBatchProcessor()

        # å»ºç«‹å¤§é‡ä»»å‹™
        large_prompt_list = [f"é€™æ˜¯ç¬¬ {i} å€‹æ¸¬è©¦æç¤ºè©" for i in range(100)]

        start_time = time.time()
        tasks = batch_processor.create_tasks(large_prompt_list)
        creation_time = time.time() - start_time

        # æ‡‰è©²åœ¨ 0.1 ç§’å…§å®Œæˆ
        self.assertLess(creation_time, 0.1)
        self.assertEqual(len(tasks), 100)

if __name__ == '__main__':
    print("ğŸ§ª åŸ·è¡Œ Stage 3 æ•´åˆæ¸¬è©¦å¥—ä»¶")
    print("=" * 50)

    # è¨­å®šæ¸¬è©¦æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--full':
        os.environ['TGW_FULL_TEST'] = 'true'
        print("âš ï¸ åŸ·è¡Œå®Œæ•´æ¸¬è©¦ï¼Œè«‹ç¢ºèª TGW å·²å•Ÿå‹•")
    else:
        print("â„¹ï¸ åŸ·è¡ŒåŸºç¤æ¸¬è©¦ï¼Œä½¿ç”¨ --full åƒæ•¸åŸ·è¡Œå®Œæ•´æ¸¬è©¦")

    # å»ºç«‹æ¸¬è©¦å¥—ä»¶
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()

    # æ·»åŠ æ¸¬è©¦é¡åˆ¥
    suite.addTests(loader.loadTestsFromTestCase(TestStage3Integration))
    suite.addTests(loader.loadTestsFromTestCase(TestStage3Performance))

    # åŸ·è¡Œæ¸¬è©¦
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # é¡¯ç¤ºçµæœ
    if result.wasSuccessful():
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
        sys.exit(0)
    else:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {len(result.failures)} å€‹å¤±æ•—, {len(result.errors)} å€‹éŒ¯èª¤")
        sys.exit(1)
