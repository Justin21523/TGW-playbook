# stages/stage3-default-notebook/batch_processor.py
"""
TGW Batch Generation & Quality Control Tool
æ‰¹æ¬¡ç”Ÿæˆèˆ‡å“è³ªæ§åˆ¶å°ˆç”¨å·¥å…·

åŠŸèƒ½ï¼š
- æ‰¹æ¬¡æ–‡æœ¬ç”Ÿæˆ
- å“è³ªè©•ä¼°å’Œç¯©é¸
- ä¸€è‡´æ€§æª¢æŸ¥
- è¼¸å‡ºæ ¼å¼æ¨™æº–åŒ–
- ç”Ÿæˆçµæœæ¯”è¼ƒå’Œæ’åº
"""

import json
import time
import asyncio
import aiohttp
import requests
import re
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, asdict
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from datetime import datetime
import statistics


@dataclass
class GenerationTask:
    """ç”Ÿæˆä»»å‹™"""

    id: str
    prompt: str
    parameters: Dict[str, Any]
    template_name: str = ""
    priority: int = 1
    metadata: Dict[str, Any] = None


@dataclass
class GenerationResult:
    """ç”Ÿæˆçµæœ"""

    task_id: str
    prompt: str
    generated_text: str
    parameters: Dict[str, Any]
    generation_time: float
    token_count: int
    quality_score: float
    metadata: Dict[str, Any]
    timestamp: str
    success: bool
    error_message: str = ""


class QualityMetrics:
    """å“è³ªè©•ä¼°æŒ‡æ¨™"""

    @staticmethod
    def calculate_readability(text: str) -> float:
        """è¨ˆç®—å¯è®€æ€§åˆ†æ•¸ (0-1)"""
        if not text.strip():
            return 0.0

        sentences = re.split(r"[.!?ã€‚ï¼ï¼Ÿ]", text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            return 0.0

        # å¹³å‡å¥å­é•·åº¦
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)

        # æ¨™é»ç¬¦è™Ÿå¯†åº¦
        punctuation_count = len(re.findall(r"[.!?ã€‚ï¼ï¼Ÿ,ï¼Œ;ï¼›:]", text))
        punctuation_density = punctuation_count / max(len(text.split()), 1)

        # å¯è®€æ€§åˆ†æ•¸ (åŸºæ–¼å¥å­é•·åº¦å’Œæ¨™é»ä½¿ç”¨)
        readability = (
            max(0, min(1, 1 - (avg_sentence_length - 15) / 30)) * 0.7
            + min(punctuation_density * 10, 1) * 0.3
        )

        return round(readability, 3)

    @staticmethod
    def calculate_coherence(text: str) -> float:
        """è¨ˆç®—é€£è²«æ€§åˆ†æ•¸ (0-1)"""
        if not text.strip():
            return 0.0

        sentences = re.split(r"[.!?ã€‚ï¼ï¼Ÿ]", text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) < 2:
            return 0.5  # å–®å¥é»˜èªä¸­ç­‰é€£è²«æ€§

        # æª¢æŸ¥é€£æ¥è©ä½¿ç”¨
        connectors = [
            "å› æ­¤",
            "æ‰€ä»¥",
            "ä½†æ˜¯",
            "ç„¶è€Œ",
            "æ­¤å¤–",
            "å¦å¤–",
            "é¦–å…ˆ",
            "å…¶æ¬¡",
            "æœ€å¾Œ",
            "ç¸½ä¹‹",
        ]
        connector_usage = sum(1 for connector in connectors if connector in text)
        connector_score = min(connector_usage / max(len(sentences) * 0.3, 1), 1)

        # æª¢æŸ¥é‡è¤‡æ¦‚å¿µ (ç°¡å–®è©å½™é‡è¤‡åˆ†æ)
        words = text.split()
        unique_words = set(words)
        repetition_ratio = len(unique_words) / max(len(words), 1)

        # é€£è²«æ€§åˆ†æ•¸
        coherence = connector_score * 0.6 + repetition_ratio * 0.4

        return round(coherence, 3)

    @staticmethod
    def calculate_completeness(text: str, prompt: str) -> float:
        """è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸ (0-1)"""
        if not text.strip():
            return 0.0

        # åŸºæ–¼é•·åº¦çš„å®Œæ•´æ€§ (ç›¸å°æ–¼æç¤ºè©)
        length_ratio = len(text) / max(
            len(prompt) * 2, 100
        )  # æœŸæœ›è¼¸å‡ºæ˜¯æç¤ºè©çš„2å€ä»¥ä¸Š
        length_score = min(length_ratio, 1)

        # æª¢æŸ¥æ˜¯å¦æœ‰æ˜ç¢ºçµå°¾
        has_conclusion = any(
            ending in text for ending in ["ç¸½çµ", "çµè«–", "ç¶œä¸Š", "æœ€å¾Œ", "ã€‚", "ï¼"]
        )
        conclusion_score = 1.0 if has_conclusion else 0.5

        # å®Œæ•´æ€§åˆ†æ•¸
        completeness = length_score * 0.7 + conclusion_score * 0.3

        return round(completeness, 3)

    @staticmethod
    def calculate_relevance(text: str, prompt: str) -> float:
        """è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸ (0-1)"""
        if not text.strip() or not prompt.strip():
            return 0.0

        # æå–æç¤ºè©ä¸­çš„é—œéµè©
        prompt_words = set(re.findall(r"\b\w+\b", prompt.lower()))
        text_words = set(re.findall(r"\b\w+\b", text.lower()))

        # è¨ˆç®—è©å½™é‡ç–Šåº¦
        if prompt_words:
            overlap_ratio = len(prompt_words & text_words) / len(prompt_words)
        else:
            overlap_ratio = 0.0

        return round(overlap_ratio, 3)


class TGWBatchGenerator:
    """TGW æ‰¹æ¬¡ç”Ÿæˆå™¨"""

    def __init__(
        self,
        api_base: str = "http://localhost:5000",
        results_dir: str = "./results/batch_generation",
        max_concurrent: int = 3,
    ):
        self.api_base = api_base
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.max_concurrent = max_concurrent
        self.quality_metrics = QualityMetrics()

    def create_task(
        self,
        prompt: str,
        parameters: Dict[str, Any] = None,
        template_name: str = "",
        priority: int = 1,
        metadata: Dict[str, Any] = None,
    ) -> GenerationTask:
        """å»ºç«‹ç”Ÿæˆä»»å‹™"""

        if parameters is None:
            parameters = {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_new_tokens": 512,
                "repetition_penalty": 1.1,
            }

        if metadata is None:
            metadata = {}

        # ç”Ÿæˆä»»å‹™ ID
        task_id = hashlib.md5(f"{prompt}_{time.time()}".encode()).hexdigest()[:8]

        return GenerationTask(
            id=task_id,
            prompt=prompt,
            parameters=parameters,
            template_name=template_name,
            priority=priority,
            metadata=metadata,
        )

    def generate_single(self, task: GenerationTask) -> GenerationResult:
        """åŸ·è¡Œå–®å€‹ç”Ÿæˆä»»å‹™"""

        start_time = time.time()

        try:
            response = requests.post(
                f"{self.api_base}/v1/completions",
                json={"prompt": task.prompt, **task.parameters},
                headers={"Content-Type": "application/json"},
                timeout=60,  # 60ç§’è¶…æ™‚
            )

            generation_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("choices", [{}])[0].get("text", "")

                # è¨ˆç®—å“è³ªåˆ†æ•¸
                quality_score = self._calculate_overall_quality(
                    generated_text, task.prompt
                )

                # ä¼°ç®— token æ•¸é‡
                token_count = len(generated_text.split())  # ç°¡åŒ–ä¼°ç®—

                return GenerationResult(
                    task_id=task.id,
                    prompt=task.prompt,
                    generated_text=generated_text,
                    parameters=task.parameters,
                    generation_time=generation_time,
                    token_count=token_count,
                    quality_score=quality_score,
                    metadata=task.metadata,
                    timestamp=datetime.now().isoformat(),
                    success=True,
                )
            else:
                return GenerationResult(
                    task_id=task.id,
                    prompt=task.prompt,
                    generated_text="",
                    parameters=task.parameters,
                    generation_time=generation_time,
                    token_count=0,
                    quality_score=0.0,
                    metadata=task.metadata,
                    timestamp=datetime.now().isoformat(),
                    success=False,
                    error_message=f"API éŒ¯èª¤: {response.status_code}",
                )

        except Exception as e:
            generation_time = time.time() - start_time
            return GenerationResult(
                task_id=task.id,
                prompt=task.prompt,
                generated_text="",
                parameters=task.parameters,
                generation_time=generation_time,
                token_count=0,
                quality_score=0.0,
                metadata=task.metadata,
                timestamp=datetime.now().isoformat(),
                success=False,
                error_message=str(e),
            )

    def batch_generate(
        self,
        tasks: List[GenerationTask],
        quality_threshold: float = 0.6,
        max_retries: int = 2,
    ) -> Dict[str, List[GenerationResult]]:
        """æ‰¹æ¬¡ç”ŸæˆåŸ·è¡Œ"""

        print(f"ğŸš€ é–‹å§‹æ‰¹æ¬¡ç”Ÿæˆ {len(tasks)} å€‹ä»»å‹™...")

        # æŒ‰å„ªå…ˆç´šæ’åº
        tasks.sort(key=lambda t: t.priority, reverse=True)

        all_results = []
        successful_results = []
        failed_results = []
        low_quality_results = []

        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡ŒåŸ·è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_task = {
                executor.submit(self.generate_single, task): task for task in tasks
            }

            for i, future in enumerate(as_completed(future_to_task)):
                task = future_to_task[future]

                try:
                    result = future.result()
                    all_results.append(result)

                    print(f"  å®Œæˆ {i+1}/{len(tasks)} - ä»»å‹™ {task.id}")

                    if result.success:
                        if result.quality_score >= quality_threshold:
                            successful_results.append(result)
                            print(f"    âœ… æˆåŠŸ (å“è³ª: {result.quality_score:.3f})")
                        else:
                            low_quality_results.append(result)
                            print(f"    âš ï¸ å“è³ªä¸ä½³ (å“è³ª: {result.quality_score:.3f})")
                    else:
                        failed_results.append(result)
                        print(f"    âŒ å¤±æ•—: {result.error_message}")

                except Exception as e:
                    print(f"    ğŸ’¥ åŸ·è¡Œç•°å¸¸: {e}")

        # å“è³ªä¸ä½³çš„çµæœé‡è©¦
        if low_quality_results and max_retries > 0:
            print(f"ğŸ”„ é‡è©¦ {len(low_quality_results)} å€‹å“è³ªä¸ä½³çš„ä»»å‹™...")

            retry_tasks = []
            for result in low_quality_results:
                # èª¿æ•´åƒæ•¸é€²è¡Œé‡è©¦
                retry_params = result.parameters.copy()
                retry_params["temperature"] = max(
                    0.3, retry_params.get("temperature", 0.7) - 0.2
                )

                retry_task = GenerationTask(
                    id=f"{result.task_id}_retry",
                    prompt=result.prompt,
                    parameters=retry_params,
                    metadata=result.metadata,
                )
                retry_tasks.append(retry_task)

            retry_results = self.batch_generate(
                retry_tasks, quality_threshold, max_retries - 1
            )
            successful_results.extend(retry_results["successful"])

        # æ•´ç†çµæœ
        results = {
            "successful": successful_results,
            "failed": failed_results,
            "low_quality": low_quality_results,
            "all": all_results,
            "summary": {
                "total_tasks": len(tasks),
                "successful_count": len(successful_results),
                "failed_count": len(failed_results),
                "low_quality_count": len(low_quality_results),
                "success_rate": len(successful_results) / len(tasks) * 100,
                "avg_quality_score": (
                    statistics.mean([r.quality_score for r in successful_results])
                    if successful_results
                    else 0
                ),
                "total_generation_time": sum(r.generation_time for r in all_results),
                "avg_generation_time": (
                    statistics.mean([r.generation_time for r in all_results])
                    if all_results
                    else 0
                ),
            },
        }

        # å„²å­˜çµæœ
        self._save_batch_results(results)

        print(f"âœ… æ‰¹æ¬¡ç”Ÿæˆå®Œæˆï¼")
        print(
            f"   æˆåŠŸ: {len(successful_results)}/{len(tasks)} ({results['summary']['success_rate']:.1f}%)"
        )
        print(f"   å¹³å‡å“è³ªåˆ†æ•¸: {results['summary']['avg_quality_score']:.3f}")

        return results

    def _calculate_overall_quality(self, text: str, prompt: str) -> float:
        """è¨ˆç®—æ•´é«”å“è³ªåˆ†æ•¸"""

        readability = self.quality_metrics.calculate_readability(text)
        coherence = self.quality_metrics.calculate_coherence(text)
        completeness = self.quality_metrics.calculate_completeness(text, prompt)
        relevance = self.quality_metrics.calculate_relevance(text, prompt)

        # åŠ æ¬Šå¹³å‡
        overall_quality = (
            readability * 0.25
            + coherence * 0.25
            + completeness * 0.25
            + relevance * 0.25
        )

        return round(overall_quality, 3)

    def _save_batch_results(self, results: Dict):
        """å„²å­˜æ‰¹æ¬¡çµæœ"""
        timestamp = int(time.time())
        filepath = self.results_dir / f"batch_results_{timestamp}.json"

        # å°‡ dataclass è½‰æ›ç‚º dict
        serializable_results = {
            "successful": [asdict(r) for r in results["successful"]],
            "failed": [asdict(r) for r in results["failed"]],
            "low_quality": [asdict(r) for r in results["low_quality"]],
            "all": [asdict(r) for r in results["all"]],
            "summary": results["summary"],
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ æ‰¹æ¬¡çµæœå·²å„²å­˜: {filepath}")

    def create_content_variations(
        self,
        base_prompt: str,
        variation_params: List[Dict[str, Any]],
        variation_names: List[str] = None,
    ) -> List[GenerationTask]:
        """å»ºç«‹å…§å®¹è®ŠåŒ–ä»»å‹™"""

        if variation_names is None:
            variation_names = [f"è®ŠåŒ–_{i+1}" for i in range(len(variation_params))]

        tasks = []

        for i, params in enumerate(variation_params):
            task = self.create_task(
                prompt=base_prompt,
                parameters=params,
                template_name=variation_names[i],
                metadata={"variation_type": variation_names[i]},
            )
            tasks.append(task)

        return tasks

    def filter_results_by_quality(
        self, results: List[GenerationResult], min_quality: float = 0.7
    ) -> List[GenerationResult]:
        """æŒ‰å“è³ªç¯©é¸çµæœ"""

        filtered = [r for r in results if r.quality_score >= min_quality]

        print(
            f"ğŸ” å“è³ªç¯©é¸: {len(filtered)}/{len(results)} ç¬¦åˆæ¨™æº– (>= {min_quality})"
        )

        return filtered

    def rank_results_by_quality(
        self, results: List[GenerationResult]
    ) -> List[GenerationResult]:
        """æŒ‰å“è³ªæ’åºçµæœ"""

        ranked = sorted(results, key=lambda r: r.quality_score, reverse=True)

        print("ğŸ† å“è³ªæ’å:")
        for i, result in enumerate(ranked[:5]):  # åªé¡¯ç¤ºå‰ 5 å
            print(f"  {i+1}. ä»»å‹™ {result.task_id}: {result.quality_score:.3f}")

        return ranked

    def generate_quality_report(self, results: List[GenerationResult]) -> str:
        """ç”Ÿæˆå“è³ªå ±å‘Š"""

        if not results:
            return "ç„¡çµæœå¯åˆ†æ"

        # å“è³ªçµ±è¨ˆ
        quality_scores = [r.quality_score for r in results if r.success]

        if not quality_scores:
            return "ç„¡æˆåŠŸçµæœå¯åˆ†æ"

        avg_quality = statistics.mean(quality_scores)
        median_quality = statistics.median(quality_scores)
        min_quality = min(quality_scores)
        max_quality = max(quality_scores)
        std_quality = statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0

        # ç”Ÿæˆæ™‚é–“çµ±è¨ˆ
        generation_times = [r.generation_time for r in results if r.success]
        avg_time = statistics.mean(generation_times) if generation_times else 0

        # Token çµ±è¨ˆ
        token_counts = [r.token_count for r in results if r.success]
        avg_tokens = statistics.mean(token_counts) if token_counts else 0

        report = f"""
# æ‰¹æ¬¡ç”Ÿæˆå“è³ªå ±å‘Š

## æ•´é«”çµ±è¨ˆ
- **ç¸½ä»»å‹™æ•¸**: {len(results)}
- **æˆåŠŸä»»å‹™æ•¸**: {len([r for r in results if r.success])}
- **æˆåŠŸç‡**: {len([r for r in results if r.success])/len(results)*100:.1f}%

## å“è³ªåˆ†æ
- **å¹³å‡å“è³ªåˆ†æ•¸**: {avg_quality:.3f}
- **ä¸­ä½æ•¸å“è³ªåˆ†æ•¸**: {median_quality:.3f}
- **æœ€é«˜å“è³ªåˆ†æ•¸**: {max_quality:.3f}
- **æœ€ä½å“è³ªåˆ†æ•¸**: {min_quality:.3f}
- **å“è³ªæ¨™æº–å·®**: {std_quality:.3f}

## æ€§èƒ½æŒ‡æ¨™
- **å¹³å‡ç”Ÿæˆæ™‚é–“**: {avg_time:.2f} ç§’
- **å¹³å‡ Token æ•¸**: {avg_tokens:.0f}

## å“è³ªåˆ†ä½ˆ
"""

        # å“è³ªåˆ†ä½ˆçµ±è¨ˆ
        quality_ranges = [
            (0.9, 1.0, "å„ªç§€"),
            (0.8, 0.9, "è‰¯å¥½"),
            (0.7, 0.8, "å°šå¯"),
            (0.6, 0.7, "éœ€æ”¹å–„"),
            (0.0, 0.6, "å“è³ªä¸ä½³"),
        ]

        for min_q, max_q, label in quality_ranges:
            count = len([q for q in quality_scores if min_q <= q < max_q])
            percentage = count / len(quality_scores) * 100
            report += f"- **{label}** ({min_q:.1f}-{max_q:.1f}): {count} å€‹ ({percentage:.1f}%)\n"

        # æœ€ä½³çµæœå±•ç¤º
        best_results = sorted(results, key=lambda r: r.quality_score, reverse=True)[:3]

        report += "\n## æœ€ä½³çµæœç¯„ä¾‹\n"
        for i, result in enumerate(best_results):
            if result.success:
                report += f"""
### ç¬¬ {i+1} å (å“è³ªåˆ†æ•¸: {result.quality_score:.3f})
**æç¤ºè©**: {result.prompt[:100]}...
**ç”Ÿæˆå…§å®¹**: {result.generated_text[:200]}...
**ç”Ÿæˆæ™‚é–“**: {result.generation_time:.2f} ç§’
"""

        return report


def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    print("âš¡ TGW Batch Generation & Quality Control Tool")
    print("=" * 60)

    # åˆå§‹åŒ–æ‰¹æ¬¡ç”Ÿæˆå™¨
    generator = TGWBatchGenerator(max_concurrent=2)  # é™ä½ä¸¦è¡Œæ•¸é¿å…éè¼‰

    print("ğŸ”§ å»ºç«‹ç¯„ä¾‹ç”Ÿæˆä»»å‹™...")

    # å»ºç«‹å¤šå€‹æ¸¬è©¦ä»»å‹™
    test_prompts = [
        "è«‹å¯«ä¸€ç¯‡é—œæ–¼æ©Ÿå™¨å­¸ç¿’åŸºç¤æ¦‚å¿µçš„æŠ€è¡“æ–‡ç« ï¼ŒåŒ…å«ç›£ç£å­¸ç¿’å’Œç„¡ç›£ç£å­¸ç¿’çš„å·®ç•°ã€‚",
        "è¨­è¨ˆä¸€å€‹ Python å‡½æ•¸ä¾†è¨ˆç®—è²»æ³¢ç´å¥‘æ•¸åˆ—ï¼Œè¦æ±‚åŒ…å«å®Œæ•´çš„è¨»è§£å’ŒéŒ¯èª¤è™•ç†ã€‚",
        "åˆ†æç•¶å‰äººå·¥æ™ºæ…§ç™¼å±•çš„ä¸‰å€‹ä¸»è¦è¶¨å‹¢ï¼Œä¸¦èªªæ˜å°æœªä¾†ç¤¾æœƒçš„å½±éŸ¿ã€‚",
        "å‰µä½œä¸€å€‹ç§‘å¹»çŸ­æ•…äº‹ï¼ŒèƒŒæ™¯è¨­å®šåœ¨ 2050 å¹´çš„æ™ºæ…§åŸå¸‚ã€‚",
        "æ’°å¯«ä¸€ä»½å°ˆæ¡ˆç®¡ç†æœ€ä½³å¯¦å‹™æŒ‡å—ï¼Œé©ç”¨æ–¼ä¸­å°å‹è»Ÿé«”é–‹ç™¼åœ˜éšŠã€‚",
    ]

    # å»ºç«‹ä¸åŒåƒæ•¸çµ„åˆçš„ä»»å‹™
    parameter_variations = [
        {"temperature": 0.3, "top_p": 0.8, "max_new_tokens": 400},  # ä¿å®ˆåƒæ•¸
        {"temperature": 0.7, "top_p": 0.9, "max_new_tokens": 500},  # å¹³è¡¡åƒæ•¸
        {"temperature": 0.9, "top_p": 0.95, "max_new_tokens": 600},  # å‰µæ„åƒæ•¸
    ]

    all_tasks = []

    for i, prompt in enumerate(test_prompts):
        # ç‚ºæ¯å€‹æç¤ºè©å»ºç«‹å¤šå€‹è®ŠåŒ–ç‰ˆæœ¬
        variations = generator.create_content_variations(
            base_prompt=prompt,
            variation_params=parameter_variations,
            variation_names=["ä¿å®ˆé¢¨æ ¼", "å¹³è¡¡é¢¨æ ¼", "å‰µæ„é¢¨æ ¼"],
        )

        # æ·»åŠ å…ƒæ•¸æ“š
        for j, task in enumerate(variations):
            task.metadata.update(
                {"prompt_category": f"category_{i+1}", "variation_index": j}
            )
            task.priority = 3 - j  # ä¿å®ˆé¢¨æ ¼å„ªå…ˆç´šæœ€é«˜

        all_tasks.extend(variations)

    print(f"ğŸ“‹ å·²å»ºç«‹ {len(all_tasks)} å€‹ç”Ÿæˆä»»å‹™")

    # åŸ·è¡Œæ‰¹æ¬¡ç”Ÿæˆ
    results = generator.batch_generate(
        tasks=all_tasks, quality_threshold=0.6, max_retries=1
    )

    # åˆ†æçµæœ
    print("\nğŸ“Š çµæœåˆ†æ:")
    print(f"æˆåŠŸç”Ÿæˆ: {len(results['successful'])} å€‹")
    print(f"å¤±æ•—: {len(results['failed'])} å€‹")
    print(f"å“è³ªä¸ä½³: {len(results['low_quality'])} å€‹")

    if results["successful"]:
        # å“è³ªç¯©é¸å’Œæ’åº
        high_quality = generator.filter_results_by_quality(
            results["successful"], min_quality=0.7
        )

        ranked_results = generator.rank_results_by_quality(results["successful"])

        # ç”Ÿæˆå“è³ªå ±å‘Š
        quality_report = generator.generate_quality_report(results["all"])

        # å„²å­˜å“è³ªå ±å‘Š
        timestamp = int(time.time())
        report_path = generator.results_dir / f"quality_report_{timestamp}.md"

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(quality_report)

        print(f"\nğŸ“„ å“è³ªå ±å‘Šå·²å„²å­˜: {report_path}")

        # é¡¯ç¤ºæ‘˜è¦
        print("\nğŸ“ˆ å“è³ªå ±å‘Šæ‘˜è¦:")
        print(
            quality_report[:500] + "..."
            if len(quality_report) > 500
            else quality_report
        )

    print(f"\nâœ… æ‰¹æ¬¡ç”Ÿæˆå®Œæˆï¼æ‰€æœ‰çµæœå„²å­˜åœ¨: {generator.results_dir}")


if __name__ == "__main__":
    main()
